{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b455368-72ec-4cc6-88bc-af03959d66ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f33360d-8495-4712-b02f-e6817570a7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sensors, IoT devices, social networks, and online transactions all generate data that needs to be monitored constantly and acted upon quickly. As a result, the need for large-scale, real-time stream processing is more evident than ever before. This tutorial module introduces Structured Streaming, the main model for handling streaming datasets in Apache Spark. *In Structured Streaming, a data stream is treated as a table that is being continuously appended*. This leads to a stream processing model that is very similar to a batch processing model. You express your streaming computation as a standard batch-like query as on a static table, but Spark runs it as an incremental query on the unbounded input table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb8e9e5-2a2d-472c-a787-d4ddc098cc1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "Databricks has sample event data as files in`/databricks-datasets/structured-streaming/events/` to use to build a Structured Streaming application. Let's take a look at the contents of this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0383d6bd-ba38-4fe0-bdc3-de65492f19bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-0.json</td><td>file-0.json</td><td>72530</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-1.json</td><td>file-1.json</td><td>72961</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-10.json</td><td>file-10.json</td><td>73025</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-11.json</td><td>file-11.json</td><td>72999</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-12.json</td><td>file-12.json</td><td>72987</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-13.json</td><td>file-13.json</td><td>73006</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-14.json</td><td>file-14.json</td><td>73003</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-15.json</td><td>file-15.json</td><td>73007</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-16.json</td><td>file-16.json</td><td>72978</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-17.json</td><td>file-17.json</td><td>73008</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-18.json</td><td>file-18.json</td><td>73002</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-19.json</td><td>file-19.json</td><td>73014</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-2.json</td><td>file-2.json</td><td>73007</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-20.json</td><td>file-20.json</td><td>72987</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-21.json</td><td>file-21.json</td><td>72983</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-22.json</td><td>file-22.json</td><td>73009</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-23.json</td><td>file-23.json</td><td>72985</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-24.json</td><td>file-24.json</td><td>73020</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-25.json</td><td>file-25.json</td><td>72980</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-26.json</td><td>file-26.json</td><td>73002</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-27.json</td><td>file-27.json</td><td>73013</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-28.json</td><td>file-28.json</td><td>73005</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-29.json</td><td>file-29.json</td><td>72977</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-3.json</td><td>file-3.json</td><td>72996</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-30.json</td><td>file-30.json</td><td>73009</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-31.json</td><td>file-31.json</td><td>73008</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-32.json</td><td>file-32.json</td><td>72982</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-33.json</td><td>file-33.json</td><td>73033</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-34.json</td><td>file-34.json</td><td>72985</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-35.json</td><td>file-35.json</td><td>72974</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-36.json</td><td>file-36.json</td><td>73013</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-37.json</td><td>file-37.json</td><td>72989</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-38.json</td><td>file-38.json</td><td>72999</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-39.json</td><td>file-39.json</td><td>73013</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-4.json</td><td>file-4.json</td><td>72992</td><td>1596690605000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-40.json</td><td>file-40.json</td><td>72986</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-41.json</td><td>file-41.json</td><td>73019</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-42.json</td><td>file-42.json</td><td>72986</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-43.json</td><td>file-43.json</td><td>72990</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-44.json</td><td>file-44.json</td><td>73018</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-45.json</td><td>file-45.json</td><td>72997</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-46.json</td><td>file-46.json</td><td>72991</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-47.json</td><td>file-47.json</td><td>73009</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-48.json</td><td>file-48.json</td><td>72993</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-49.json</td><td>file-49.json</td><td>73496</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-5.json</td><td>file-5.json</td><td>72998</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-6.json</td><td>file-6.json</td><td>72997</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-7.json</td><td>file-7.json</td><td>73022</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-8.json</td><td>file-8.json</td><td>72997</td><td>1596690606000</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-9.json</td><td>file-9.json</td><td>72970</td><td>1596690606000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-0.json",
         "file-0.json",
         72530,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-1.json",
         "file-1.json",
         72961,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-10.json",
         "file-10.json",
         73025,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-11.json",
         "file-11.json",
         72999,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-12.json",
         "file-12.json",
         72987,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-13.json",
         "file-13.json",
         73006,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-14.json",
         "file-14.json",
         73003,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-15.json",
         "file-15.json",
         73007,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-16.json",
         "file-16.json",
         72978,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-17.json",
         "file-17.json",
         73008,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-18.json",
         "file-18.json",
         73002,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-19.json",
         "file-19.json",
         73014,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-2.json",
         "file-2.json",
         73007,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-20.json",
         "file-20.json",
         72987,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-21.json",
         "file-21.json",
         72983,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-22.json",
         "file-22.json",
         73009,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-23.json",
         "file-23.json",
         72985,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-24.json",
         "file-24.json",
         73020,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-25.json",
         "file-25.json",
         72980,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-26.json",
         "file-26.json",
         73002,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-27.json",
         "file-27.json",
         73013,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-28.json",
         "file-28.json",
         73005,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-29.json",
         "file-29.json",
         72977,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-3.json",
         "file-3.json",
         72996,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-30.json",
         "file-30.json",
         73009,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-31.json",
         "file-31.json",
         73008,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-32.json",
         "file-32.json",
         72982,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-33.json",
         "file-33.json",
         73033,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-34.json",
         "file-34.json",
         72985,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-35.json",
         "file-35.json",
         72974,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-36.json",
         "file-36.json",
         73013,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-37.json",
         "file-37.json",
         72989,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-38.json",
         "file-38.json",
         72999,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-39.json",
         "file-39.json",
         73013,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-4.json",
         "file-4.json",
         72992,
         1596690605000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-40.json",
         "file-40.json",
         72986,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-41.json",
         "file-41.json",
         73019,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-42.json",
         "file-42.json",
         72986,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-43.json",
         "file-43.json",
         72990,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-44.json",
         "file-44.json",
         73018,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-45.json",
         "file-45.json",
         72997,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-46.json",
         "file-46.json",
         72991,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-47.json",
         "file-47.json",
         73009,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-48.json",
         "file-48.json",
         72993,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-49.json",
         "file-49.json",
         73496,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-5.json",
         "file-5.json",
         72998,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-6.json",
         "file-6.json",
         72997,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-7.json",
         "file-7.json",
         73022,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-8.json",
         "file-8.json",
         72997,
         1596690606000
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/events/file-9.json",
         "file-9.json",
         72970,
         1596690606000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /databricks-datasets/structured-streaming/events/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "800c1fb2-af0f-4ec1-8b52-f2bd67e210b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Each line in the file contains a JSON record with two fields: `time` and `action`.\n",
    "\n",
    "```\n",
    "{\"time\":1469501675,\"action\":\"Open\"}\n",
    "{\"time\":1469501678,\"action\":\"Close\"}{\"time\":1469501680,\"action\":\"Open\"}{\"time\":1469501685,\"action\":\"Open\"}{\"time\":1469501686,\"action\":\"Open\"}{\"time\":1469501689,\"action\":\"Open\"}{\"time\":1469501691,\"action\":\"Open\"}{\"time\":1469501694,\"action\":\"Open\"}{\"time\":1469501696,\"action\":\"Close\"}{\"time\":1469501702,\"action\":\"Open\"}{\"time\":1469501703,\"action\":\"Open\"}{\"time\":1469501704,\"action\":\"Open\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d864bfc6-dfcd-470f-a2a2-c95ce4890111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initialize the stream\n",
    "\n",
    "Since the sample data is just a static set of files, you can emulate a stream from them by reading one file at a time, in the chronological order in which they were created:\n",
    "\n",
    "```python\n",
    "streamingInputDF = (\n",
    "  spark.readStream\\\n",
    "    .schema(jsonSchema)               # Set the schema of the JSON data\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .json(inputPath)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f641d009-97c8-4582-ba08-098e57fedd23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db8fdfaa-90ef-400d-a0fe-d3f196f764a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inputPath = \"/databricks-datasets/structured-streaming/events/\"\n",
    "\n",
    "# Define the schema to speed up processing\n",
    "jsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n",
    "\n",
    "streamingInputDF = (\n",
    "  spark.readStream\\\n",
    "    .schema(jsonSchema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n",
    "streamingCountsDF = (\n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.action,\n",
    "      window(streamingInputDF.time, \"1 hour\"))\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "861be523-07e6-4f6c-8d24-46ddae32ae0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You start a streaming computation by defining a sink and starting it. In our case, to query the counts interactively, set the complete set of 1 hour counts to be in an in-memory table. The command below essentially simulates a time series generator in real time, using the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12592f46-9e1c-42fc-b217-fc0dcbc23fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5892011683551895>, line 7\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m      2\u001B[0m   streamingCountsDF\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m)        \u001B[38;5;66;03m# memory = store in-memory table (for testing only)\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mqueryName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcounts\u001B[39m\u001B[38;5;124m\"\u001B[39m)     \u001B[38;5;66;03m# counts = name of the in-memory table\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# complete = all the counts should be in the table\u001B[39;00m\n",
       "\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[1;32m      8\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:648\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\n",
       "\u001B[1;32m    640\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    641\u001B[0m     path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    646\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptionalPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    647\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StreamingQuery:\n",
       "\u001B[0;32m--> 648\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_internal(\n",
       "\u001B[1;32m    649\u001B[0m         path\u001B[38;5;241m=\u001B[39mpath,\n",
       "\u001B[1;32m    650\u001B[0m         tableName\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    651\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m,\n",
       "\u001B[1;32m    652\u001B[0m         outputMode\u001B[38;5;241m=\u001B[39moutputMode,\n",
       "\u001B[1;32m    653\u001B[0m         partitionBy\u001B[38;5;241m=\u001B[39mpartitionBy,\n",
       "\u001B[1;32m    654\u001B[0m         queryName\u001B[38;5;241m=\u001B[39mqueryName,\n",
       "\u001B[1;32m    655\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions,\n",
       "\u001B[1;32m    656\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:610\u001B[0m, in \u001B[0;36mDataStreamWriter._start_internal\u001B[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m    607\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_proto\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m tableName\n",
       "\u001B[1;32m    609\u001B[0m cmd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_stream\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m--> 610\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd)\n",
       "\u001B[1;32m    612\u001B[0m start_result \u001B[38;5;241m=\u001B[39m cast(\n",
       "\u001B[1;32m    613\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mWriteStreamOperationStartResult, properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite_stream_operation_start_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m    614\u001B[0m )\n",
       "\u001B[1;32m    615\u001B[0m query \u001B[38;5;241m=\u001B[39m StreamingQuery(\n",
       "\u001B[1;32m    616\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n",
       "\u001B[1;32m    617\u001B[0m     queryId\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mquery_id\u001B[38;5;241m.\u001B[39mid,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    621\u001B[0m     name\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mif\u001B[39;00m start_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    622\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1297\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1295\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1296\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1297\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1298\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1299\u001B[0m )\n",
       "\u001B[1;32m   1300\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1301\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1752\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1755\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1756\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1757\u001B[0m     ):\n",
       "\u001B[1;32m   1758\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1759\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1729\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2146\u001B[0m                 )\n",
       "\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2150\u001B[0m                 info,\n",
       "\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED] Trigger type ProcessingTime is not supported for this cluster type.\n",
       "Use a different trigger type e.g. AvailableNow, Once. SQLSTATE: 0A000\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.infiniteStreamingTriggerNotSupportedError(QueryCompilationErrors.scala:5600)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:454)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:295)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4043)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3299)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1447)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED] Trigger type ProcessingTime is not supported for this cluster type.\nUse a different trigger type e.g. AvailableNow, Once. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.infiniteStreamingTriggerNotSupportedError(QueryCompilationErrors.scala:5600)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:454)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:295)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4043)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3299)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1447)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       },
       "metadata": {
        "errorSummary": "[INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED] Trigger type ProcessingTime is not supported for this cluster type.\nUse a different trigger type e.g. AvailableNow, Once. SQLSTATE: 0A000"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "sqlState": "0A000",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.infiniteStreamingTriggerNotSupportedError(QueryCompilationErrors.scala:5600)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:454)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:295)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4043)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3299)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1447)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-5892011683551895>, line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m   streamingCountsDF\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m)        \u001B[38;5;66;03m# memory = store in-memory table (for testing only)\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mqueryName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcounts\u001B[39m\u001B[38;5;124m\"\u001B[39m)     \u001B[38;5;66;03m# counts = name of the in-memory table\u001B[39;00m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# complete = all the counts should be in the table\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      8\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:648\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    641\u001B[0m     path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    646\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptionalPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    647\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StreamingQuery:\n\u001B[0;32m--> 648\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_internal(\n\u001B[1;32m    649\u001B[0m         path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[1;32m    650\u001B[0m         tableName\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    651\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m,\n\u001B[1;32m    652\u001B[0m         outputMode\u001B[38;5;241m=\u001B[39moutputMode,\n\u001B[1;32m    653\u001B[0m         partitionBy\u001B[38;5;241m=\u001B[39mpartitionBy,\n\u001B[1;32m    654\u001B[0m         queryName\u001B[38;5;241m=\u001B[39mqueryName,\n\u001B[1;32m    655\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions,\n\u001B[1;32m    656\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:610\u001B[0m, in \u001B[0;36mDataStreamWriter._start_internal\u001B[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m    607\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_proto\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m tableName\n\u001B[1;32m    609\u001B[0m cmd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_stream\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m--> 610\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd)\n\u001B[1;32m    612\u001B[0m start_result \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m    613\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mWriteStreamOperationStartResult, properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite_stream_operation_start_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    614\u001B[0m )\n\u001B[1;32m    615\u001B[0m query \u001B[38;5;241m=\u001B[39m StreamingQuery(\n\u001B[1;32m    616\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n\u001B[1;32m    617\u001B[0m     queryId\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mquery_id\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    621\u001B[0m     name\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mif\u001B[39;00m start_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    622\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1297\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1295\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1296\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1297\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1298\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1299\u001B[0m )\n\u001B[1;32m   1300\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1301\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1752\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1755\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1756\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1757\u001B[0m     ):\n\u001B[1;32m   1758\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1759\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1729\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2146\u001B[0m                 )\n\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2150\u001B[0m                 info,\n\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED] Trigger type ProcessingTime is not supported for this cluster type.\nUse a different trigger type e.g. AvailableNow, Once. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.infiniteStreamingTriggerNotSupportedError(QueryCompilationErrors.scala:5600)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:454)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:295)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4043)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3299)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1447)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only)\n",
    "    .queryName(\"counts\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e5e84f0-602f-4741-bbf1-1a72b10091fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`query` is a handle to the streaming query named `counts` that is running in the background. This query continuously picks up files and updates the windowed counts. The command window reports the status of the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72f7c79b-7488-4729-a785-9aa67e205c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sadly, we cannot proceed in databricks because we cannot start a continuously updating streaming process. But if we could, we could periodically query the counts aggregation:\n",
    "\n",
    "```sql\n",
    "%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action\n",
    "```\n",
    "\n",
    "The query would change every time you execute it to reflect the action count based on the input stream of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c4f82d-4b8a-46f8-ba53-9ca019596726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5892011683551889,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5-Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}