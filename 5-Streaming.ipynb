{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b455368-72ec-4cc6-88bc-af03959d66ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f33360d-8495-4712-b02f-e6817570a7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sensors, IoT devices, social networks, and online transactions all generate data that needs to be monitored constantly and acted upon quickly. As a result, the need for large-scale, real-time stream processing is more evident than ever before. This tutorial module introduces Structured Streaming, the main model for handling streaming datasets in Apache Spark. *In Structured Streaming, a data stream is treated as a table that is being continuously appended*. This leads to a stream processing model that is very similar to a batch processing model. You express your streaming computation as a standard batch-like query as on a static table, but Spark runs it as an incremental query on the unbounded input table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb8e9e5-2a2d-472c-a787-d4ddc098cc1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "Databricks has sample event data as files in`/databricks-datasets/structured-streaming/events/` to use to build a Structured Streaming application. Let's take a look at the contents of this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0383d6bd-ba38-4fe0-bdc3-de65492f19bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This would work in databricks: \n",
    "\n",
    "    %fs ls /databricks-datasets/structured-streaming/events/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_geo.csv  file-18.json  file-28.json  file-38.json\tfile-48.json\n",
      "file-0.json   file-19.json  file-29.json  file-39.json\tfile-49.json\n",
      "file-1.json   file-2.json   file-3.json   file-4.json\tfile-5.json\n",
      "file-10.json  file-20.json  file-30.json  file-40.json\tfile-6.json\n",
      "file-11.json  file-21.json  file-31.json  file-41.json\tfile-7.json\n",
      "file-12.json  file-22.json  file-32.json  file-42.json\tfile-8.json\n",
      "file-13.json  file-23.json  file-33.json  file-43.json\tfile-9.json\n",
      "file-14.json  file-24.json  file-34.json  file-44.json\tiot_devices.json\n",
      "file-15.json  file-25.json  file-35.json  file-45.json\tpeople.json\n",
      "file-16.json  file-26.json  file-36.json  file-46.json\n",
      "file-17.json  file-27.json  file-37.json  file-47.json\n"
     ]
    }
   ],
   "source": [
    "!ls datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "800c1fb2-af0f-4ec1-8b52-f2bd67e210b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Each line in the file contains a JSON record with two fields: `time` and `action`.\n",
    "\n",
    "```\n",
    "{\"time\":1469501675,\"action\":\"Open\"}\n",
    "{\"time\":1469501678,\"action\":\"Close\"}{\"time\":1469501680,\"action\":\"Open\"}{\"time\":1469501685,\"action\":\"Open\"}{\"time\":1469501686,\"action\":\"Open\"}{\"time\":1469501689,\"action\":\"Open\"}{\"time\":1469501691,\"action\":\"Open\"}{\"time\":1469501694,\"action\":\"Open\"}{\"time\":1469501696,\"action\":\"Close\"}{\"time\":1469501702,\"action\":\"Open\"}{\"time\":1469501703,\"action\":\"Open\"}{\"time\":1469501704,\"action\":\"Open\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d864bfc6-dfcd-470f-a2a2-c95ce4890111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initialize the stream\n",
    "\n",
    "Since the sample data is just a static set of files, you can emulate a stream from them by reading one file at a time, in the chronological order in which they were created:\n",
    "\n",
    "```python\n",
    "streamingInputDF = (\n",
    "  spark.readStream\\\n",
    "    .schema(jsonSchema)               # Set the schema of the JSON data\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .json(inputPath)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f641d009-97c8-4582-ba08-098e57fedd23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db8fdfaa-90ef-400d-a0fe-d3f196f764a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inputPath = \"/databricks-datasets/structured-streaming/events/\"\n",
    "\n",
    "# Define the schema to speed up processing\n",
    "jsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n",
    "\n",
    "streamingInputDF = (\n",
    "  spark.readStream\\\n",
    "    .schema(jsonSchema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n",
    "streamingCountsDF = (\n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.action,\n",
    "      window(streamingInputDF.time, \"1 hour\"))\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "861be523-07e6-4f6c-8d24-46ddae32ae0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You start a streaming computation by defining a sink and starting it. In our case, to query the counts interactively, set the complete set of 1 hour counts to be in an in-memory table. The command below essentially simulates a time series generator in real time, using the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12592f46-9e1c-42fc-b217-fc0dcbc23fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-5892011683551895>, line 7\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m (\n",
       "\u001b[1;32m      2\u001b[0m   streamingCountsDF\n",
       "\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mwriteStream\n",
       "\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \u001b[38;5;66;03m# memory = store in-memory table (for testing only)\u001b[39;00m\n",
       "\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounts\u001b[39m\u001b[38;5;124m\"\u001b[39m)     \u001b[38;5;66;03m# counts = name of the in-memory table\u001b[39;00m\n",
       "\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# complete = all the counts should be in the table\u001b[39;00m\n",
       "\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
       "\u001b[1;32m      8\u001b[0m )\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:648\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n",
       "\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\n",
       "\u001b[1;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
       "\u001b[1;32m    641\u001b[0m     path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m    646\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptionalPrimitiveType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m    647\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StreamingQuery:\n",
       "\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_internal(\n",
       "\u001b[1;32m    649\u001b[0m         path\u001b[38;5;241m=\u001b[39mpath,\n",
       "\u001b[1;32m    650\u001b[0m         tableName\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "\u001b[1;32m    651\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n",
       "\u001b[1;32m    652\u001b[0m         outputMode\u001b[38;5;241m=\u001b[39moutputMode,\n",
       "\u001b[1;32m    653\u001b[0m         partitionBy\u001b[38;5;241m=\u001b[39mpartitionBy,\n",
       "\u001b[1;32m    654\u001b[0m         queryName\u001b[38;5;241m=\u001b[39mqueryName,\n",
       "\u001b[1;32m    655\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions,\n",
       "\u001b[1;32m    656\u001b[0m     )\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:610\u001b[0m, in \u001b[0;36mDataStreamWriter._start_internal\u001b[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n",
       "\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_proto\u001b[38;5;241m.\u001b[39mtable_name \u001b[38;5;241m=\u001b[39m tableName\n",
       "\u001b[1;32m    609\u001b[0m cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_stream\u001b[38;5;241m.\u001b[39mcommand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n",
       "\u001b[0;32m--> 610\u001b[0m (_, properties, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mexecute_command(cmd)\n",
       "\u001b[1;32m    612\u001b[0m start_result \u001b[38;5;241m=\u001b[39m cast(\n",
       "\u001b[1;32m    613\u001b[0m     pb2\u001b[38;5;241m.\u001b[39mWriteStreamOperationStartResult, properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite_stream_operation_start_result\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
       "\u001b[1;32m    614\u001b[0m )\n",
       "\u001b[1;32m    615\u001b[0m query \u001b[38;5;241m=\u001b[39m StreamingQuery(\n",
       "\u001b[1;32m    616\u001b[0m     session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session,\n",
       "\u001b[1;32m    617\u001b[0m     queryId\u001b[38;5;241m=\u001b[39mstart_result\u001b[38;5;241m.\u001b[39mquery_id\u001b[38;5;241m.\u001b[39mid,\n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m    621\u001b[0m     name\u001b[38;5;241m=\u001b[39mstart_result\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mif\u001b[39;00m start_result\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "\u001b[1;32m    622\u001b[0m )\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1297\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command, observations, extra_request_metadata)\u001b[0m\n",
       "\u001b[1;32m   1295\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n",
       "\u001b[1;32m   1296\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n",
       "\u001b[0;32m-> 1297\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch(\n",
       "\u001b[1;32m   1298\u001b[0m     req, observations \u001b[38;5;129;01mor\u001b[39;00m {}, extra_request_metadata\n",
       "\u001b[1;32m   1299\u001b[0m )\n",
       "\u001b[1;32m   1300\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n",
       "\u001b[1;32m   1301\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[0m\n",
       "\u001b[1;32m   1752\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n",
       "\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n",
       "\u001b[0;32m-> 1755\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n",
       "\u001b[1;32m   1756\u001b[0m         req, observations, extra_request_metadata \u001b[38;5;129;01mor\u001b[39;00m [], progress\u001b[38;5;241m=\u001b[39mprogress\n",
       "\u001b[1;32m   1757\u001b[0m     ):\n",
       "\u001b[1;32m   1758\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n",
       "\u001b[1;32m   1759\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, extra_request_metadata, progress)\u001b[0m\n",
       "\u001b[1;32m   1729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n",
       "\u001b[1;32m   1730\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
       "\u001b[0;32m-> 1731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n",
       "\u001b[1;32m   2045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n",
       "\u001b[0;32m-> 2047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error(error)\n",
       "\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
       "\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n",
       "\u001b[1;32m   2134\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n",
       "\u001b[1;32m   2135\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython versions in the Spark Connect client and server are different. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[1;32m   2136\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo execute user-defined functions, client and server should have the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m   2145\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[1;32m   2146\u001b[0m                 )\n",
       "\u001b[1;32m   2147\u001b[0m             \u001b[38;5;66;03m# END-EDGE\u001b[39;00m\n",
       "\u001b[0;32m-> 2149\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n",
       "\u001b[1;32m   2150\u001b[0m                 info,\n",
       "\u001b[1;32m   2151\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n",
       "\u001b[1;32m   2152\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n",
       "\u001b[1;32m   2153\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n",
       "\u001b[1;32m   2154\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   2157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED] Trigger type ProcessingTime is not supported for this cluster type.\n",
       "Use a different trigger type e.g. AvailableNow, Once. SQLSTATE: 0A000\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.infiniteStreamingTriggerNotSupportedError(QueryCompilationErrors.scala:5600)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:454)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:295)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4043)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3299)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1447)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED] Trigger type ProcessingTime is not supported for this cluster type.\nUse a different trigger type e.g. AvailableNow, Once. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.infiniteStreamingTriggerNotSupportedError(QueryCompilationErrors.scala:5600)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:454)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:295)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4043)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3299)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1447)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       },
       "metadata": {
        "errorSummary": "[INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED] Trigger type ProcessingTime is not supported for this cluster type.\nUse a different trigger type e.g. AvailableNow, Once. SQLSTATE: 0A000"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "sqlState": "0A000",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.infiniteStreamingTriggerNotSupportedError(QueryCompilationErrors.scala:5600)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:454)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:295)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4043)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3299)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1447)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
        "File \u001b[0;32m<command-5892011683551895>, line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m   streamingCountsDF\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mwriteStream\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \u001b[38;5;66;03m# memory = store in-memory table (for testing only)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounts\u001b[39m\u001b[38;5;124m\"\u001b[39m)     \u001b[38;5;66;03m# counts = name of the in-memory table\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# complete = all the counts should be in the table\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      8\u001b[0m )\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:648\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    641\u001b[0m     path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptionalPrimitiveType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    647\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StreamingQuery:\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_internal(\n\u001b[1;32m    649\u001b[0m         path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    650\u001b[0m         tableName\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m    652\u001b[0m         outputMode\u001b[38;5;241m=\u001b[39moutputMode,\n\u001b[1;32m    653\u001b[0m         partitionBy\u001b[38;5;241m=\u001b[39mpartitionBy,\n\u001b[1;32m    654\u001b[0m         queryName\u001b[38;5;241m=\u001b[39mqueryName,\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions,\n\u001b[1;32m    656\u001b[0m     )\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:610\u001b[0m, in \u001b[0;36mDataStreamWriter._start_internal\u001b[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_proto\u001b[38;5;241m.\u001b[39mtable_name \u001b[38;5;241m=\u001b[39m tableName\n\u001b[1;32m    609\u001b[0m cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_stream\u001b[38;5;241m.\u001b[39mcommand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m--> 610\u001b[0m (_, properties, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mexecute_command(cmd)\n\u001b[1;32m    612\u001b[0m start_result \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    613\u001b[0m     pb2\u001b[38;5;241m.\u001b[39mWriteStreamOperationStartResult, properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite_stream_operation_start_result\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    614\u001b[0m )\n\u001b[1;32m    615\u001b[0m query \u001b[38;5;241m=\u001b[39m StreamingQuery(\n\u001b[1;32m    616\u001b[0m     session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session,\n\u001b[1;32m    617\u001b[0m     queryId\u001b[38;5;241m=\u001b[39mstart_result\u001b[38;5;241m.\u001b[39mquery_id\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m     name\u001b[38;5;241m=\u001b[39mstart_result\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mif\u001b[39;00m start_result\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    622\u001b[0m )\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1297\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command, observations, extra_request_metadata)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n\u001b[1;32m   1296\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n\u001b[0;32m-> 1297\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch(\n\u001b[1;32m   1298\u001b[0m     req, observations \u001b[38;5;129;01mor\u001b[39;00m {}, extra_request_metadata\n\u001b[1;32m   1299\u001b[0m )\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[0m\n\u001b[1;32m   1752\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[0;32m-> 1755\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n\u001b[1;32m   1756\u001b[0m         req, observations, extra_request_metadata \u001b[38;5;129;01mor\u001b[39;00m [], progress\u001b[38;5;241m=\u001b[39mprogress\n\u001b[1;32m   1757\u001b[0m     ):\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[1;32m   1759\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, extra_request_metadata, progress)\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 2047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error(error)\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   2134\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m   2135\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython versions in the Spark Connect client and server are different. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2136\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo execute user-defined functions, client and server should have the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2145\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2146\u001b[0m                 )\n\u001b[1;32m   2147\u001b[0m             \u001b[38;5;66;03m# END-EDGE\u001b[39;00m\n\u001b[0;32m-> 2149\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[1;32m   2150\u001b[0m                 info,\n\u001b[1;32m   2151\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   2152\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n\u001b[1;32m   2153\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n\u001b[1;32m   2154\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
        "\u001b[0;31mAnalysisException\u001b[0m: [INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED] Trigger type ProcessingTime is not supported for this cluster type.\nUse a different trigger type e.g. AvailableNow, Once. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.infiniteStreamingTriggerNotSupportedError(QueryCompilationErrors.scala:5600)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:454)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:295)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4043)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3299)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1447)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only)\n",
    "    .queryName(\"counts\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e5e84f0-602f-4741-bbf1-1a72b10091fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`query` is a handle to the streaming query named `counts` that is running in the background. This query continuously picks up files and updates the windowed counts. The command window reports the status of the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72f7c79b-7488-4729-a785-9aa67e205c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sadly, we cannot proceed in databricks because we cannot start a continuously updating streaming process. But if we could, we could periodically query the counts aggregation:\n",
    "\n",
    "```sql\n",
    "%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action\n",
    "```\n",
    "\n",
    "The query would change every time you execute it to reflect the action count based on the input stream of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c4f82d-4b8a-46f8-ba53-9ca019596726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5892011683551889,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5-Streaming",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
