{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b455368-72ec-4cc6-88bc-af03959d66ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f33360d-8495-4712-b02f-e6817570a7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sensors, IoT devices, social networks, and online transactions all generate data that needs to be monitored constantly and acted upon quickly. As a result, the need for large-scale, real-time stream processing is more evident than ever before. This tutorial module introduces Structured Streaming, the main model for handling streaming datasets in Apache Spark. *In Structured Streaming, a data stream is treated as a table that is being continuously appended*. This leads to a stream processing model that is very similar to a batch processing model. You express your streaming computation as a standard batch-like query as on a static table, but Spark runs it as an incremental query on the unbounded input table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb8e9e5-2a2d-472c-a787-d4ddc098cc1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "Databricks has sample event data as files in`/databricks-datasets/structured-streaming/events/` to use to build a Structured Streaming application. Let's take a look at the contents of this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0383d6bd-ba38-4fe0-bdc3-de65492f19bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This would work in databricks: \n",
    "\n",
    "    %fs ls /databricks-datasets/structured-streaming/events/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-0.json   file-18.json  file-27.json  file-36.json\tfile-45.json\n",
      "file-1.json   file-19.json  file-28.json  file-37.json\tfile-46.json\n",
      "file-10.json  file-2.json   file-29.json  file-38.json\tfile-47.json\n",
      "file-11.json  file-20.json  file-3.json   file-39.json\tfile-48.json\n",
      "file-12.json  file-21.json  file-30.json  file-4.json\tfile-49.json\n",
      "file-13.json  file-22.json  file-31.json  file-40.json\tfile-5.json\n",
      "file-14.json  file-23.json  file-32.json  file-41.json\tfile-6.json\n",
      "file-15.json  file-24.json  file-33.json  file-42.json\tfile-7.json\n",
      "file-16.json  file-25.json  file-34.json  file-43.json\tfile-8.json\n",
      "file-17.json  file-26.json  file-35.json  file-44.json\tfile-9.json\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "800c1fb2-af0f-4ec1-8b52-f2bd67e210b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Each line in the file contains a JSON record with two fields: `time` and `action`.\n",
    "\n",
    "```\n",
    "{\"time\":1469501675,\"action\":\"Open\"}\n",
    "{\"time\":1469501678,\"action\":\"Close\"}{\"time\":1469501680,\"action\":\"Open\"}{\"time\":1469501685,\"action\":\"Open\"}{\"time\":1469501686,\"action\":\"Open\"}{\"time\":1469501689,\"action\":\"Open\"}{\"time\":1469501691,\"action\":\"Open\"}{\"time\":1469501694,\"action\":\"Open\"}{\"time\":1469501696,\"action\":\"Close\"}{\"time\":1469501702,\"action\":\"Open\"}{\"time\":1469501703,\"action\":\"Open\"}{\"time\":1469501704,\"action\":\"Open\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d864bfc6-dfcd-470f-a2a2-c95ce4890111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initialize the stream\n",
    "\n",
    "Since the sample data is just a static set of files, you can emulate a stream from them by reading one file at a time, in the chronological order in which they were created:\n",
    "\n",
    "```python\n",
    "streamingInputDF = (\n",
    "  spark.readStream\\\n",
    "    .schema(jsonSchema)               # Set the schema of the JSON data\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .json(inputPath)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f641d009-97c8-4582-ba08-098e57fedd23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db8fdfaa-90ef-400d-a0fe-d3f196f764a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#inputPath = \"/databricks-datasets/structured-streaming/events/\" # databricks\n",
    "inputPath = \"./datasets/events/\"\n",
    "\n",
    "# Define the schema to speed up processing\n",
    "jsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n",
    "\n",
    "streamingInputDF = (\n",
    "  spark.readStream\\\n",
    "    .schema(jsonSchema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n",
    "streamingCountsDF = (\n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.action,\n",
    "      window(streamingInputDF.time, \"1 hour\"))\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "861be523-07e6-4f6c-8d24-46ddae32ae0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start the streaming job\n",
    "\n",
    "You start a streaming computation by defining a sink and starting it. In our case, to query the counts interactively, set the complete set of 1 hour counts to be in an in-memory table. The command below essentially simulates a time series generator in real time, using the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12592f46-9e1c-42fc-b217-fc0dcbc23fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only)\n",
    "    .queryName(\"counts\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e5e84f0-602f-4741-bbf1-1a72b10091fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`query` is a handle to the streaming query named `counts` that is running in the background. This query continuously picks up files and updates the windowed counts. The command window reports the status of the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72f7c79b-7488-4729-a785-9aa67e205c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Query the stream in real time\n",
    "\n",
    "We now periodically query the `counts` aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=spark.sql(\"select action, date_format(window.end, 'MMM-dd HH:mm') as time, count from counts order by time, action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+\n",
      "|action|        time|count|\n",
      "+------+------------+-----+\n",
      "| Close|Jul-26 03:00|   11|\n",
      "|  Open|Jul-26 03:00|  179|\n",
      "| Close|Jul-26 04:00|  344|\n",
      "|  Open|Jul-26 04:00| 1001|\n",
      "| Close|Jul-26 05:00|  815|\n",
      "|  Open|Jul-26 05:00|  999|\n",
      "| Close|Jul-26 06:00| 1003|\n",
      "|  Open|Jul-26 06:00| 1000|\n",
      "| Close|Jul-26 07:00| 1011|\n",
      "|  Open|Jul-26 07:00|  993|\n",
      "| Close|Jul-26 08:00|  989|\n",
      "|  Open|Jul-26 08:00| 1008|\n",
      "| Close|Jul-26 09:00|  318|\n",
      "|  Open|Jul-26 09:00|  329|\n",
      "| Close|Jul-26 14:00|  699|\n",
      "|  Open|Jul-26 14:00|  656|\n",
      "| Close|Jul-26 15:00|  994|\n",
      "|  Open|Jul-26 15:00|  991|\n",
      "| Close|Jul-26 16:00|  988|\n",
      "|  Open|Jul-26 16:00| 1020|\n",
      "+------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72f7c79b-7488-4729-a785-9aa67e205c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The query would change every time you execute it to reflect the action count based on the input stream of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5892011683551889,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5-Streaming",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
